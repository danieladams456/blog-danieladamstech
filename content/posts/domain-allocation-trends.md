---
title: "Domain Allocation Trends"
date: 2023-12-02T20:00:00-05:00
draft: true
---

I was thinking about how to best chronicle my time so far in tech showcasing the variety of contributions I have delivered. After thinking it through, I came up with the idea of a graph showing my focus between the areas of software, data, cloud, and security.

I had the look I wanted in mind but wasn't sure how to implement it. My first concent was a stacked bar graph with variable width bars depending on my time in the position. Stacked bar graphs were easy, but there did not seem to be many examples out there about how to do one with variable width bars. [The Python Graph Gallery](https://python-graph-gallery.com/) lead me to realize a [stacked area chart](https://python-graph-gallery.com/251-stacked-area-chart-with-seaborn-style/) would get me the look I was after. By increasing the number of points, the shape would asymptotically approach a variable width stacked bar graph. Repeating each entry for the number of months gave me a good look (and was logical as well!) Removing the percentile ticks on the Y axis and customizing labels and positioning on the X axis completed the graph.

![Domain allocation trends graph](https://images.danieladamstech.com/2023-daniel-adams-domain-allocation-trends.svg)
See the [Python notebook](https://github.com/danieladams456/blog-danieladamstech/blob/main/content/code-examples/domain-allocation-trends.ipynb) and [JSON definition](https://github.com/danieladams456/blog-danieladamstech/blob/main/content/code-examples/domain-allocation-trends.json) I used to make this graph.

### My Tech Journey

This is some context you won't find on my [Linkedin profile.](https://www.linkedin.com/in/danieladams15/)

My first introduction to programming when I tool AP Computer Science in 11th grade. Ever since then, I was hooked. When looking at colleges and scholarships, I came across a program with the NSA that would have been a scholarship, summer internships, and a job after college. I made it to the last round up at Fort Meade including full scope poly for TS/SCI clearance, but didn't get the offer. I still liked the cyber security angle. During college, I participated in our cyber defense team. Our team placed [2nd in the 2014 Mid-Atlantic Collegiate Cyber Defense Competition.](https://maccdc.org/maccdc-2014/) During college, I did a 8 month full-time co-op with Babcock and Wilcox mPower on their [small modular reactor](https://www.forbes.com/sites/rodadams/2017/03/13/bechtel-and-bwxt-quietly-terminate-mpower-reactor-project/) building UI prototypes for control room operators. The Nuclear Regulatory Commission was not approving projects around that era, so the next summer I moved on to Genworth with a endpoint security internship.

After college, I started the IT Leadership Development Program. This program included four 6-month rotations on different teams to broaden my horizons and help pick the area that was the best fit for roll-off. I started out in Richmond working on Box and OneDrive data loss prevention. Then I worked on the ETL team where I took fraud records from Hive and sent them to an ESB to ingest the data back into the transactional system. During this time, [Atul Saurav](https://www.linkedin.com/in/atulsaurav/) was a great mentor teaching me a lot about data. Still thinking I wanted to stay a security guy, I moved down to Raleigh for an application security rotation on the Mortgage Insurance business unit's security team. I've been here in Raleigh ever since! The cloud was a hot new topic at that time. After hearing there was a team working on AWS and they had recently launched a new EMR based analytics platform, I worked towards getting on that team. My thought was that this would be an opportunity to move security earlier into the design phase rather than trying to fix it after the fact. I created a [git repo with terraform labs](https://github.com/danieladams456/aws-labs) I worked on in the evenings to show I was serious about learning AWS. This was around the time AWS ALBs were coming out and the company was migrating off a Consul-based stack to them, so in my labs I built a [two-tier alb system](https://github.com/danieladams456/aws-labs/tree/master/dual_alb) for public and private services. When I presented my initiative to [Chris,](https://www.linkedin.com/in/christopherdsmith76/details/experience/) at that time Director of Architecture and now CTO, he gave me the opportunity to rotate on his team. At one point in the six month rotation (and trial for remaining on the team) as I was working through the various projects, he said, "as long as you keep hitting it out of the park, I'll keep giving you more stuff to try."

Roll-off time came, and I received an offer to stay on the Enterprise Architecture team. I was very thankful for the opportunity! [Mark](https://www.linkedin.com/in/mark-griffin-8760b31/) and [Mike,](https://www.linkedin.com/in/mike-lyon-13899/) the most senior individual contributors who had worked as a team of two for the previous 10 years, let me join them. It was a great learning opportunity to be around them during the early years of building out our cloud environment. Guiding principles were codification of everything, strong environment parity, and cross environment/business context isolation. That team functioned as the [cloud center of excellence](https://aws.amazon.com/blogs/publicsector/what-is-cloud-center-excellence-why-should-your-organization-create-one/) and delivered big projects partnering with the various development teams. As the obviously junior member, the projects were more internal-facing platform elements, but there were some that started working on my influence skills across teams. One of those was leading the migration from DataDog to NewRelic (back when they had the strongest APM and distributed tracing offering) and then driving adoption across all our development teams. We rolled it out successfully, and years later I am still the go-to SME on performance monitoring. (Currently I lead a cross-team monitoring focus group sharing best practices and helping others level-up our monitoring across all our apps.) Other projects of interest was working with other teams (including HQ approvals) to build out our business unit's own ADFS cluster primarily for federating to AWS. This was years before IAM Identity Center, so I built a utility site allowing us to use SAML roles with CLI and SDK access. It was a server-side rendered site built on Lambda and API Gateway that parsed the SAML assertion and presented a similar screen to the main AWS sign in (though with Bootstrap styling). Users could click a role and the AWS JS browser SDK would call STS for temporary credentials. These were rendered into a cross-platform set of `aws configure` commands that could easily be pasted into a terminal to refresh the appropriate profile. I also worked on a proposed enterprise-wide API fronting a tokenization provider. Circumstances changed and that project was implemented at another time with another solution, but it was fun writing that service and thinking through the use cases. That was the position I stayed in the longest at 3 years, trying to learn by doing and osmosis from the experienced people around me.

Our cloud footprint kept growing and reached the point where we needed a dedicated team to manage it. I was selected to bootstrap the new team. We started out as just the cloud team, but around that time (early 2021) also started building a cloud data engineering team. At the end of my three years in the position, we had grown to a team size of 9 FTEs and 6 contractors including a Director of Cloud + Data and Data Engineering manager. There have been jokes made about the number of people needed to replace me as my switching teams was what spurred Cloud Engineering to get their own manager and further expand the team. :) During the whole growth phase for both cloud and data, I was in the technical interview loop for every individual contributor candidate. Besides the organizational, cross-training, and coordinating angle, I also dove into the technical opportunities. We follow a strict infrastructure as code practice where all AWS resources are in Terraform. This includes both our ECS/Lambda services and infrastructure pieces. I made improvements to our workflow in both cases. I reduced 24 different Terraform ECS service deploy pipeline templates into 1 "standard" service type and a python-generated multi-region version. On the infrastructure project side, I stripped our wrapper scripts to the bare minimum while increasing variable derivation flexibility and reducing ongoing maintenance of script customization. My time here was nicely split into two 18 month stints of cloud focus and data focus. I was heavily involved in the design and buildout of our Snowflake data lake. We used Talend for data integration, orchestrating the ELT process, and tracking SQL-based lineage. I built out the development processes, repo structure, CI/CD, Snowflake environment, Snowflake security model, Talend AWS infrastructure, Eventbridge patterns, MLOps pipelines, and assisted with ETL design where appropriate. One of the big wins on the CI/CD front was adding SQL linting and static analysis with [SQLFluff.](https://sqlfluff.com/) That addition to the pipeline saved hours of data engineer time by speeding up cycle time to find certain errors instead waiting for jobs to run. I had the opportunity to work closely with the data scientists by being the interface between the two teams. I worked with the Data Science leads to develop their processes, create their sandbox environment, and help with last-mile model productionalization.

Then at the start of 2024, I felt the need to get back into software. There was plenty of scripting and software-adjacent coding going on, but I was missing the all-in experience. I transitioned from a hybrid Architecture team and a Platform Cloud/Data team to a true Product team when I joined Integrations. We are responsible for our external customer interactions. We own tracking and normalization layers and then hand off to other teams who do rate quotes, orders, and other backoffice servicing functions. Today 80% of our business comes through integration channels managed by our team. Collaboration between all the teams is essential for a good customer experience as each one tackles a different angle of complexity. The integration team's mission is to maintain a consistent API to all our 1,800 B2B customers. To that end, we participate in the [MISMO](https://www.mismo.org/) standards organization, similar to the [IETF](https://www.ietf.org/) for the web. MISMO focuses on business process touch point and data interchange format standardization between all parties in the mortgage process. Our corporate values are [Excellence, Improvement, and Connection.](https://enactmi.com/enact-careers) As the face of technology to our customers, we want to be customer-obsessed, just like Amazon's first [Leadership Principle.](https://www.amazon.jobs/content/en/our-workplace/leadership-principles)

> **Customer Obsession**
>
> Leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. Although leaders pay attention to competitors, they obsess over customers.

It has been a great time so far building things that I can see our external and internal customers use, whether that's launching a new MISMO 3.5 Rate Quote API or a new query tool to provide support a unified view of our on-prem and native cloud-based integration systems. I look forward to continuing to grow in customer empathy, product mentality, technical leadership, and engineering practice.
